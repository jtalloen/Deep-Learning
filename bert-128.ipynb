{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import *\n",
    "from torch.utils.data import TensorDataset, Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "import sklearn.metrics\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:  True\n"
     ]
    }
   ],
   "source": [
    "cuda = torch.cuda.is_available()\n",
    "print(\"cuda: \", cuda)\n",
    "# num_workers = 8 if cuda else 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1304607\n",
      "111426\n"
     ]
    }
   ],
   "source": [
    "print(len(train_texts))\n",
    "print(len(val_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total length: 1304607\n",
      "num over 32: 17607\n",
      "num over 64: 878\n",
      "num over 128: 37\n",
      "max len: 648\n"
     ]
    }
   ],
   "source": [
    "counterasdf = 0\n",
    "max_len = len(train_texts[0].split())\n",
    "over_200 = 0\n",
    "over_500 = 0\n",
    "over_1000 = 0\n",
    "for s in train_texts:\n",
    "    if len(s.split()) > 32:\n",
    "        over_200 += 1\n",
    "    if len(s.split()) > 64:\n",
    "        over_500 += 1\n",
    "    if len(s.split()) > 128:\n",
    "        over_1000 += 1\n",
    "    if len(s.split()) > max_len:\n",
    "        max_len = len(s.split())\n",
    "        # print(\"=\" * 40)\n",
    "        # print(\"new max len: \" + str(max_len))\n",
    "        # print(\"=\" * 40)\n",
    "        # print(s)\n",
    "print(\"total length: \" + str(len(train_texts)))\n",
    "print(\"num over 32: \" + str(over_200))\n",
    "print(\"num over 64: \" + str(over_500))\n",
    "print(\"num over 128: \" + str(over_1000))\n",
    "print(\"max len: \" + str(max_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Dataset and DataLoader</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "an_dataset=pickle.load(open(\"../nutella/datasets/predict-importance-an.pkl\", \"rb\"))\n",
    "\n",
    "train_texts=[]\n",
    "train_labels=[]\n",
    "for x in an_dataset[\"train_dataset\"]:\n",
    "    train_texts.extend([q[\"txt\"] for q in x[\"data\"]])\n",
    "    train_labels.extend([q[\"important\"] for q in x[\"data\"]])\n",
    "\n",
    "val_texts=[]\n",
    "val_labels=[]    \n",
    "for x in an_dataset[\"val_dataset\"]:\n",
    "    val_texts.extend([q[\"txt\"] for q in x[\"data\"]])\n",
    "    val_labels.extend([q[\"important\"] for q in x[\"data\"]])    \n",
    "    \n",
    "test_texts=[]\n",
    "test_labels=[]        \n",
    "for x in an_dataset[\"test_dataset\"]:\n",
    "    test_texts.extend([q[\"txt\"] for q in x[\"data\"]])\n",
    "    test_labels.extend([q[\"important\"] for q in x[\"data\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done loading, this takes forever\n"
     ]
    }
   ],
   "source": [
    "input_ids = []\n",
    "attention_masks = []\n",
    "for s in train_texts:\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                    s,\n",
    "                    add_special_tokens=True,\n",
    "                    max_length=128,\n",
    "                    pad_to_max_length=True,\n",
    "                    return_attention_mask=True,\n",
    "                    return_tensors='pt'\n",
    "                )\n",
    "    \n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "train_input_ids = torch.cat(input_ids, dim=0)\n",
    "train_attention_masks = torch.cat(attention_masks, dim=0)\n",
    "train_labels = torch.tensor(train_labels)\n",
    "\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "for s in val_texts:\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                    s,\n",
    "                    add_special_tokens=True,\n",
    "                    max_length=128,\n",
    "                    pad_to_max_length=True,\n",
    "                    return_attention_mask=True,\n",
    "                    return_tensors='pt'\n",
    "                )\n",
    "    \n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "    \n",
    "val_input_ids = torch.cat(input_ids, dim=0)\n",
    "val_attention_masks = torch.cat(attention_masks, dim=0)\n",
    "val_labels = torch.tensor(val_labels)\n",
    "\n",
    "print('done loading, this takes forever')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(train_input_ids, train_attention_masks, train_labels)\n",
    "val_dataset = TensorDataset(val_input_ids, val_attention_masks, val_labels)\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(\n",
    "                train_dataset,\n",
    "                shuffle=True,\n",
    "                batch_size = batch_size)\n",
    "\n",
    "val_loader = DataLoader(\n",
    "                val_dataset,\n",
    "                shuffle=True,\n",
    "                batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1304607, 128])\n"
     ]
    }
   ],
   "source": [
    "print(train_input_ids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40769\n",
      "3483\n"
     ]
    }
   ],
   "source": [
    "print(len(train_loader))\n",
    "print(len(val_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Train, Test, Metrics Utility Functions</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_metrics(y_true, predicted):\n",
    "    this_results={}\n",
    "    \n",
    "    this_results[\"precision\"]=sklearn.metrics.precision_score(y_true, predicted)\n",
    "    this_results[\"recall\"]=sklearn.metrics.recall_score(y_true, predicted)    \n",
    "    this_results[\"f1\"]=sklearn.metrics.f1_score(y_true, predicted)\n",
    "    this_results[\"accuracy\"]=sklearn.metrics.accuracy_score(y_true, predicted)    \n",
    "    this_results[\"auc\"]=sklearn.metrics.roc_auc_score(y_true, predicted)    \n",
    "\n",
    "    return this_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, optimizer):\n",
    "    model.train() \n",
    "    cumulative_loss = 0.0\n",
    "    cumulative_lens = 0\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    print('='*40)\n",
    "    print(\"Training\", len(train_loader), \"batches\")\n",
    "    print('='*40)\n",
    "    \n",
    "    # start timer and start iterating\n",
    "    start_train = time.time()\n",
    "    for batch_idx, (data, mask, target) in enumerate(train_loader):       \n",
    "        optimizer.zero_grad()  \n",
    "        data, mask, target = data.to(device), mask.to(device), target.to(device)\n",
    "        \n",
    "        loss, logits = model(data, token_type_ids=None, attention_mask=mask, labels=target)\n",
    "        \n",
    "        # accumulate loss\n",
    "        cumulative_loss += loss.item()\n",
    "        cumulative_lens += 1   # len(target)\n",
    "        running_loss = cumulative_loss / cumulative_lens\n",
    "        \n",
    "        mid_train = time.time()\n",
    "        if batch_idx % 40 == 39:\n",
    "            print(\"Batch: \", batch_idx + 1)\n",
    "            print('Cumulative Time: {:.4f}s\\nLoss: {:.4f}'.format(mid_train - start_train, running_loss))\n",
    "            print('='*40)\n",
    "            \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step() \n",
    "        \n",
    "        # delete variables and empty cache\n",
    "        torch.cuda.empty_cache()\n",
    "        del data\n",
    "        del target\n",
    "        del mask\n",
    "    \n",
    "    # end timer and take average loss\n",
    "    end_train = time.time()\n",
    "    time_train = end_train - start_train\n",
    "    return time_train, running_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, test_loader):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "\n",
    "        running_loss = 0.0\n",
    "        target_total = []\n",
    "        predicted_total = []\n",
    "\n",
    "        start_test = time.time()\n",
    "        for batch_idx, (data, mask, target) in enumerate(test_loader): \n",
    "                \n",
    "            data, mask, target = data.to(device), mask.to(device), target.to(device)\n",
    "        \n",
    "            loss, logits = model(data, token_type_ids=None, attention_mask=mask, labels=target)\n",
    "\n",
    "            _, predicted = torch.max(logits, 1)\n",
    "            # print('predicted: ', predicted)\n",
    "            \n",
    "            # loss = criterion(logits, target).detach()\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            mid_test = time.time()\n",
    "            if batch_idx % 40 == 39:\n",
    "                print(\"Batch: \", batch_idx + 1)\n",
    "                print('Cumulative Time: {:.4f}s\\nLoss: {:.4f}'.format(mid_test - start_test, running_loss/batch_idx))\n",
    "                print('='*40)\n",
    "            \n",
    "            target = target.data.cpu().numpy()\n",
    "            predicted = predicted.data.cpu().numpy()\n",
    "            \n",
    "            target_total.extend(target)\n",
    "            predicted_total.extend(predicted)\n",
    "            \n",
    "            # delete variables and empty cache\n",
    "            torch.cuda.empty_cache()\n",
    "            del data\n",
    "            del mask\n",
    "            del target\n",
    "        \n",
    "        results = calc_metrics(np.array(target_total), np.array(predicted_total))\n",
    "        running_loss /= len(test_loader)\n",
    "        print('Dev Loss: ', running_loss)\n",
    "        print('Results', results)\n",
    "        return running_loss, results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Hyperparameters and Runtime</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef2b2a6c9e294ed58de39f34aa5cd529",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=385.0, style=ProgressStyle(description_â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# PARAMETERS \n",
    "\n",
    "epochs = 5\n",
    "learningRate = 2e-5\n",
    "weightDecay = 0.00004\n",
    "momentum = 0.9\n",
    "\n",
    "batch_size = 32\n",
    "num_layers = 5\n",
    "\n",
    "# train_data = MyDataset(encoded_train, y_train, train_lens)\n",
    "# val_data = MyDataset(encoded_val, y_val, val_lens)\n",
    "# train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size, collate_fn = collate)\n",
    "# val_loader = DataLoader(val_data, shuffle=False, batch_size=16, collate_fn = collate)\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\", num_labels=2, output_attentions=False, output_hidden_states=False) \n",
    "device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "total_steps = len(train_loader) * epochs\n",
    "optimizer = AdamW(model.parameters(), lr=learningRate, eps=1e-8) # weightDecay?\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0,\n",
    "                                            num_training_steps = total_steps)\n",
    "# scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:  40\n",
      "Cumulative Time: 5.1672s\n",
      "Loss: 0.6076\n",
      "========================================\n",
      "Batch:  80\n",
      "Cumulative Time: 10.3889s\n",
      "Loss: 0.6011\n",
      "========================================\n",
      "Batch:  120\n",
      "Cumulative Time: 15.6429s\n",
      "Loss: 0.5956\n",
      "========================================\n",
      "Batch:  160\n",
      "Cumulative Time: 20.9123s\n",
      "Loss: 0.5909\n",
      "========================================\n",
      "Batch:  200\n",
      "Cumulative Time: 26.1127s\n",
      "Loss: 0.5915\n",
      "========================================\n",
      "Batch:  240\n",
      "Cumulative Time: 31.3923s\n",
      "Loss: 0.5929\n",
      "========================================\n",
      "Batch:  280\n",
      "Cumulative Time: 36.6947s\n",
      "Loss: 0.5899\n",
      "========================================\n",
      "Batch:  320\n",
      "Cumulative Time: 41.9859s\n",
      "Loss: 0.5887\n",
      "========================================\n",
      "Batch:  360\n",
      "Cumulative Time: 47.2876s\n",
      "Loss: 0.5866\n",
      "========================================\n",
      "Batch:  400\n",
      "Cumulative Time: 52.5584s\n",
      "Loss: 0.5864\n",
      "========================================\n",
      "Batch:  440\n",
      "Cumulative Time: 57.8780s\n",
      "Loss: 0.5869\n",
      "========================================\n",
      "Batch:  480\n",
      "Cumulative Time: 63.1883s\n",
      "Loss: 0.5855\n",
      "========================================\n",
      "Batch:  520\n",
      "Cumulative Time: 68.4996s\n",
      "Loss: 0.5863\n",
      "========================================\n",
      "Batch:  560\n",
      "Cumulative Time: 73.7668s\n",
      "Loss: 0.5868\n",
      "========================================\n",
      "Batch:  600\n",
      "Cumulative Time: 79.1046s\n",
      "Loss: 0.5865\n",
      "========================================\n",
      "Batch:  640\n",
      "Cumulative Time: 84.4212s\n",
      "Loss: 0.5869\n",
      "========================================\n",
      "Batch:  680\n",
      "Cumulative Time: 89.7871s\n",
      "Loss: 0.5877\n",
      "========================================\n",
      "Batch:  720\n",
      "Cumulative Time: 95.1089s\n",
      "Loss: 0.5884\n",
      "========================================\n",
      "Batch:  760\n",
      "Cumulative Time: 100.4717s\n",
      "Loss: 0.5896\n",
      "========================================\n",
      "Batch:  800\n",
      "Cumulative Time: 105.8320s\n",
      "Loss: 0.5899\n",
      "========================================\n",
      "Batch:  840\n",
      "Cumulative Time: 111.1803s\n",
      "Loss: 0.5902\n",
      "========================================\n",
      "Batch:  880\n",
      "Cumulative Time: 116.5321s\n",
      "Loss: 0.5908\n",
      "========================================\n",
      "Batch:  920\n",
      "Cumulative Time: 121.8113s\n",
      "Loss: 0.5901\n",
      "========================================\n",
      "Batch:  960\n",
      "Cumulative Time: 127.1734s\n",
      "Loss: 0.5907\n",
      "========================================\n",
      "Batch:  1000\n",
      "Cumulative Time: 132.5223s\n",
      "Loss: 0.5892\n",
      "========================================\n",
      "Batch:  1040\n",
      "Cumulative Time: 137.8712s\n",
      "Loss: 0.5894\n",
      "========================================\n",
      "Batch:  1080\n",
      "Cumulative Time: 143.1814s\n",
      "Loss: 0.5899\n",
      "========================================\n",
      "Batch:  1120\n",
      "Cumulative Time: 148.5581s\n",
      "Loss: 0.5898\n",
      "========================================\n",
      "Batch:  1160\n",
      "Cumulative Time: 153.9234s\n",
      "Loss: 0.5898\n",
      "========================================\n",
      "Batch:  1200\n",
      "Cumulative Time: 159.3057s\n",
      "Loss: 0.5901\n",
      "========================================\n",
      "Batch:  1240\n",
      "Cumulative Time: 164.5938s\n",
      "Loss: 0.5903\n",
      "========================================\n",
      "Batch:  1280\n",
      "Cumulative Time: 169.9406s\n",
      "Loss: 0.5904\n",
      "========================================\n",
      "Batch:  1320\n",
      "Cumulative Time: 175.3028s\n",
      "Loss: 0.5905\n",
      "========================================\n",
      "Batch:  1360\n",
      "Cumulative Time: 180.6494s\n",
      "Loss: 0.5905\n",
      "========================================\n",
      "Batch:  1400\n",
      "Cumulative Time: 185.9806s\n",
      "Loss: 0.5907\n",
      "========================================\n",
      "Batch:  1440\n",
      "Cumulative Time: 191.3667s\n",
      "Loss: 0.5907\n",
      "========================================\n",
      "Batch:  1480\n",
      "Cumulative Time: 196.7298s\n",
      "Loss: 0.5909\n",
      "========================================\n",
      "Batch:  1520\n",
      "Cumulative Time: 202.1032s\n",
      "Loss: 0.5913\n",
      "========================================\n",
      "Batch:  1560\n",
      "Cumulative Time: 207.4485s\n",
      "Loss: 0.5913\n",
      "========================================\n",
      "Batch:  1600\n",
      "Cumulative Time: 212.7239s\n",
      "Loss: 0.5913\n",
      "========================================\n",
      "Batch:  1640\n",
      "Cumulative Time: 218.1146s\n",
      "Loss: 0.5916\n",
      "========================================\n",
      "Batch:  1680\n",
      "Cumulative Time: 223.4605s\n",
      "Loss: 0.5916\n",
      "========================================\n",
      "Batch:  1720\n",
      "Cumulative Time: 228.8067s\n",
      "Loss: 0.5915\n",
      "========================================\n",
      "Batch:  1760\n",
      "Cumulative Time: 234.1756s\n",
      "Loss: 0.5914\n",
      "========================================\n",
      "Batch:  1800\n",
      "Cumulative Time: 239.5108s\n",
      "Loss: 0.5914\n",
      "========================================\n",
      "Batch:  1840\n",
      "Cumulative Time: 244.8577s\n",
      "Loss: 0.5916\n",
      "========================================\n",
      "Batch:  1880\n",
      "Cumulative Time: 250.1960s\n",
      "Loss: 0.5919\n",
      "========================================\n",
      "Batch:  1920\n",
      "Cumulative Time: 255.4878s\n",
      "Loss: 0.5919\n",
      "========================================\n",
      "Batch:  1960\n",
      "Cumulative Time: 260.8353s\n",
      "Loss: 0.5920\n",
      "========================================\n",
      "Batch:  2000\n",
      "Cumulative Time: 266.2031s\n",
      "Loss: 0.5920\n",
      "========================================\n",
      "Batch:  2040\n",
      "Cumulative Time: 271.5710s\n",
      "Loss: 0.5920\n",
      "========================================\n",
      "Batch:  2080\n",
      "Cumulative Time: 276.9373s\n",
      "Loss: 0.5920\n",
      "========================================\n",
      "Batch:  2120\n",
      "Cumulative Time: 282.2554s\n",
      "Loss: 0.5922\n",
      "========================================\n",
      "Batch:  2160\n",
      "Cumulative Time: 287.6101s\n",
      "Loss: 0.5925\n",
      "========================================\n",
      "Batch:  2200\n",
      "Cumulative Time: 292.9858s\n",
      "Loss: 0.5923\n",
      "========================================\n",
      "Batch:  2240\n",
      "Cumulative Time: 298.3653s\n",
      "Loss: 0.5927\n",
      "========================================\n",
      "Batch:  2280\n",
      "Cumulative Time: 303.6153s\n",
      "Loss: 0.5932\n",
      "========================================\n",
      "Batch:  2320\n",
      "Cumulative Time: 308.9889s\n",
      "Loss: 0.5934\n",
      "========================================\n",
      "Batch:  2360\n",
      "Cumulative Time: 314.3520s\n",
      "Loss: 0.5931\n",
      "========================================\n",
      "Batch:  2400\n",
      "Cumulative Time: 319.7620s\n",
      "Loss: 0.5932\n",
      "========================================\n",
      "Batch:  2440\n",
      "Cumulative Time: 325.1006s\n",
      "Loss: 0.5935\n",
      "========================================\n",
      "Batch:  2480\n",
      "Cumulative Time: 330.4910s\n",
      "Loss: 0.5932\n",
      "========================================\n",
      "Batch:  2520\n",
      "Cumulative Time: 335.8441s\n",
      "Loss: 0.5934\n",
      "========================================\n",
      "Batch:  2560\n",
      "Cumulative Time: 341.1915s\n",
      "Loss: 0.5934\n",
      "========================================\n",
      "Batch:  2600\n",
      "Cumulative Time: 346.5715s\n",
      "Loss: 0.5936\n",
      "========================================\n",
      "Batch:  2640\n",
      "Cumulative Time: 351.8886s\n",
      "Loss: 0.5936\n",
      "========================================\n",
      "Batch:  2680\n",
      "Cumulative Time: 357.2873s\n",
      "Loss: 0.5933\n",
      "========================================\n",
      "Batch:  2720\n",
      "Cumulative Time: 362.6487s\n",
      "Loss: 0.5932\n",
      "========================================\n",
      "Batch:  2760\n",
      "Cumulative Time: 368.0387s\n",
      "Loss: 0.5932\n",
      "========================================\n",
      "Batch:  2800\n",
      "Cumulative Time: 373.3619s\n",
      "Loss: 0.5933\n",
      "========================================\n",
      "Batch:  2840\n",
      "Cumulative Time: 378.7673s\n",
      "Loss: 0.5932\n",
      "========================================\n",
      "Batch:  2880\n",
      "Cumulative Time: 384.1214s\n",
      "Loss: 0.5934\n",
      "========================================\n",
      "Batch:  2920\n",
      "Cumulative Time: 389.4721s\n",
      "Loss: 0.5932\n",
      "========================================\n",
      "Batch:  2960\n",
      "Cumulative Time: 394.7305s\n",
      "Loss: 0.5932\n",
      "========================================\n",
      "Batch:  3000\n",
      "Cumulative Time: 400.1060s\n",
      "Loss: 0.5933\n",
      "========================================\n",
      "Batch:  3040\n",
      "Cumulative Time: 405.4779s\n",
      "Loss: 0.5931\n",
      "========================================\n",
      "Batch:  3080\n",
      "Cumulative Time: 410.8407s\n",
      "Loss: 0.5931\n",
      "========================================\n",
      "Batch:  3120\n",
      "Cumulative Time: 416.1996s\n",
      "Loss: 0.5931\n",
      "========================================\n",
      "Batch:  3160\n",
      "Cumulative Time: 421.5046s\n",
      "Loss: 0.5928\n",
      "========================================\n",
      "Batch:  3200\n",
      "Cumulative Time: 426.8404s\n",
      "Loss: 0.5928\n",
      "========================================\n",
      "Batch:  3240\n",
      "Cumulative Time: 432.2078s\n",
      "Loss: 0.5927\n",
      "========================================\n",
      "Batch:  3280\n",
      "Cumulative Time: 437.5665s\n",
      "Loss: 0.5927\n",
      "========================================\n",
      "Batch:  3320\n",
      "Cumulative Time: 442.8315s\n",
      "Loss: 0.5927\n",
      "========================================\n",
      "Batch:  3360\n",
      "Cumulative Time: 448.1993s\n",
      "Loss: 0.5928\n",
      "========================================\n",
      "Batch:  3400\n",
      "Cumulative Time: 453.5979s\n",
      "Loss: 0.5929\n",
      "========================================\n",
      "Batch:  3440\n",
      "Cumulative Time: 458.9769s\n",
      "Loss: 0.5932\n",
      "========================================\n",
      "Batch:  3480\n",
      "Cumulative Time: 464.2924s\n",
      "Loss: 0.5930\n",
      "========================================\n",
      "Dev Loss:  0.5929935712120227\n",
      "Results {'precision': 0.615844305253739, 'recall': 0.46661016949152545, 'f1': 0.5309400484901918, 'accuracy': 0.694416024985192, 'auc': 0.6475950770453378}\n"
     ]
    }
   ],
   "source": [
    "val_loss, val_results = test_model(model, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Training 40769 batches\n",
      "========================================\n",
      "Batch:  40\n",
      "Cumulative Time: 13.7506s\n",
      "Loss: 0.0206\n",
      "========================================\n",
      "Batch:  80\n",
      "Cumulative Time: 27.6174s\n",
      "Loss: 0.0204\n",
      "========================================\n",
      "Batch:  120\n",
      "Cumulative Time: 41.6213s\n",
      "Loss: 0.0203\n",
      "========================================\n",
      "Batch:  160\n",
      "Cumulative Time: 55.8562s\n",
      "Loss: 0.0201\n",
      "========================================\n",
      "Batch:  200\n",
      "Cumulative Time: 69.9744s\n",
      "Loss: 0.0200\n",
      "========================================\n",
      "Batch:  240\n",
      "Cumulative Time: 84.1135s\n",
      "Loss: 0.0198\n",
      "========================================\n",
      "Batch:  280\n",
      "Cumulative Time: 98.0268s\n",
      "Loss: 0.0197\n",
      "========================================\n",
      "Batch:  320\n",
      "Cumulative Time: 112.3212s\n",
      "Loss: 0.0197\n",
      "========================================\n",
      "Batch:  360\n",
      "Cumulative Time: 126.3675s\n",
      "Loss: 0.0196\n",
      "========================================\n",
      "Batch:  400\n",
      "Cumulative Time: 140.3790s\n",
      "Loss: 0.0196\n",
      "========================================\n",
      "Batch:  440\n",
      "Cumulative Time: 154.3063s\n",
      "Loss: 0.0196\n",
      "========================================\n",
      "Batch:  480\n",
      "Cumulative Time: 168.1930s\n",
      "Loss: 0.0195\n",
      "========================================\n",
      "Batch:  520\n",
      "Cumulative Time: 182.0903s\n",
      "Loss: 0.0195\n",
      "========================================\n",
      "Batch:  560\n",
      "Cumulative Time: 196.0516s\n",
      "Loss: 0.0195\n",
      "========================================\n",
      "Batch:  600\n",
      "Cumulative Time: 210.1490s\n",
      "Loss: 0.0194\n",
      "========================================\n",
      "Batch:  640\n",
      "Cumulative Time: 224.3054s\n",
      "Loss: 0.0194\n",
      "========================================\n",
      "Batch:  680\n",
      "Cumulative Time: 238.4594s\n",
      "Loss: 0.0194\n",
      "========================================\n",
      "Batch:  720\n",
      "Cumulative Time: 252.5236s\n",
      "Loss: 0.0194\n",
      "========================================\n",
      "Batch:  760\n",
      "Cumulative Time: 266.6374s\n",
      "Loss: 0.0194\n",
      "========================================\n",
      "Batch:  800\n",
      "Cumulative Time: 280.8630s\n",
      "Loss: 0.0194\n",
      "========================================\n",
      "Batch:  840\n",
      "Cumulative Time: 295.2797s\n",
      "Loss: 0.0194\n",
      "========================================\n",
      "Batch:  880\n",
      "Cumulative Time: 309.7142s\n",
      "Loss: 0.0194\n",
      "========================================\n",
      "Batch:  920\n",
      "Cumulative Time: 324.0739s\n",
      "Loss: 0.0194\n",
      "========================================\n",
      "Batch:  960\n",
      "Cumulative Time: 338.4389s\n",
      "Loss: 0.0194\n",
      "========================================\n",
      "Batch:  1000\n",
      "Cumulative Time: 352.7886s\n",
      "Loss: 0.0193\n",
      "========================================\n",
      "Batch:  1040\n",
      "Cumulative Time: 367.1465s\n",
      "Loss: 0.0193\n",
      "========================================\n",
      "Batch:  1080\n",
      "Cumulative Time: 381.5804s\n",
      "Loss: 0.0193\n",
      "========================================\n",
      "Batch:  1120\n",
      "Cumulative Time: 396.0056s\n",
      "Loss: 0.0193\n",
      "========================================\n",
      "Batch:  1160\n",
      "Cumulative Time: 410.4418s\n",
      "Loss: 0.0193\n",
      "========================================\n",
      "Batch:  1200\n",
      "Cumulative Time: 424.7485s\n",
      "Loss: 0.0193\n",
      "========================================\n",
      "Batch:  1240\n",
      "Cumulative Time: 439.1120s\n",
      "Loss: 0.0193\n",
      "========================================\n",
      "Batch:  1280\n",
      "Cumulative Time: 453.4486s\n",
      "Loss: 0.0193\n",
      "========================================\n",
      "Batch:  1320\n",
      "Cumulative Time: 467.8931s\n",
      "Loss: 0.0193\n",
      "========================================\n",
      "Batch:  1360\n",
      "Cumulative Time: 482.2240s\n",
      "Loss: 0.0193\n",
      "========================================\n",
      "Batch:  1400\n",
      "Cumulative Time: 496.5641s\n",
      "Loss: 0.0193\n",
      "========================================\n",
      "Batch:  1440\n",
      "Cumulative Time: 511.0500s\n",
      "Loss: 0.0193\n",
      "========================================\n",
      "Batch:  1480\n",
      "Cumulative Time: 525.4756s\n",
      "Loss: 0.0193\n",
      "========================================\n",
      "Batch:  1520\n",
      "Cumulative Time: 539.9197s\n",
      "Loss: 0.0193\n",
      "========================================\n",
      "Batch:  1560\n",
      "Cumulative Time: 554.3879s\n",
      "Loss: 0.0192\n",
      "========================================\n",
      "Batch:  1600\n",
      "Cumulative Time: 568.8485s\n",
      "Loss: 0.0192\n",
      "========================================\n",
      "Batch:  1640\n",
      "Cumulative Time: 583.2346s\n",
      "Loss: 0.0192\n",
      "========================================\n",
      "Batch:  1680\n",
      "Cumulative Time: 597.6857s\n",
      "Loss: 0.0192\n",
      "========================================\n",
      "Batch:  1720\n",
      "Cumulative Time: 612.1474s\n",
      "Loss: 0.0192\n",
      "========================================\n",
      "Batch:  1760\n",
      "Cumulative Time: 626.5812s\n",
      "Loss: 0.0192\n",
      "========================================\n",
      "Batch:  1800\n",
      "Cumulative Time: 640.9699s\n",
      "Loss: 0.0192\n",
      "========================================\n",
      "Batch:  1840\n",
      "Cumulative Time: 655.4178s\n",
      "Loss: 0.0192\n",
      "========================================\n",
      "Batch:  1880\n",
      "Cumulative Time: 669.8549s\n",
      "Loss: 0.0192\n",
      "========================================\n",
      "Batch:  1920\n",
      "Cumulative Time: 684.2827s\n",
      "Loss: 0.0192\n",
      "========================================\n",
      "Batch:  1960\n",
      "Cumulative Time: 698.6992s\n",
      "Loss: 0.0192\n",
      "========================================\n",
      "Batch:  2000\n",
      "Cumulative Time: 713.0270s\n",
      "Loss: 0.0192\n",
      "========================================\n",
      "Batch:  2040\n",
      "Cumulative Time: 727.4399s\n",
      "Loss: 0.0192\n",
      "========================================\n",
      "Batch:  2080\n",
      "Cumulative Time: 741.6726s\n",
      "Loss: 0.0192\n",
      "========================================\n",
      "Batch:  2120\n",
      "Cumulative Time: 756.1568s\n",
      "Loss: 0.0192\n",
      "========================================\n",
      "Batch:  2160\n",
      "Cumulative Time: 770.6244s\n",
      "Loss: 0.0192\n",
      "========================================\n",
      "Batch:  2200\n",
      "Cumulative Time: 785.0801s\n",
      "Loss: 0.0192\n",
      "========================================\n",
      "Batch:  2240\n",
      "Cumulative Time: 799.4806s\n",
      "Loss: 0.0192\n",
      "========================================\n",
      "Batch:  2280\n",
      "Cumulative Time: 813.9266s\n",
      "Loss: 0.0192\n",
      "========================================\n",
      "Batch:  2320\n",
      "Cumulative Time: 828.3398s\n",
      "Loss: 0.0192\n",
      "========================================\n",
      "Batch:  2360\n",
      "Cumulative Time: 842.7981s\n",
      "Loss: 0.0192\n",
      "========================================\n",
      "Batch:  2400\n",
      "Cumulative Time: 857.2337s\n",
      "Loss: 0.0192\n",
      "========================================\n",
      "Batch:  2440\n",
      "Cumulative Time: 871.6842s\n",
      "Loss: 0.0192\n",
      "========================================\n",
      "Batch:  2480\n",
      "Cumulative Time: 885.6764s\n",
      "Loss: 0.0192\n",
      "========================================\n",
      "Batch:  2520\n",
      "Cumulative Time: 899.7578s\n",
      "Loss: 0.0191\n",
      "========================================\n",
      "Batch:  2560\n",
      "Cumulative Time: 913.7381s\n",
      "Loss: 0.0191\n",
      "========================================\n",
      "Batch:  2600\n",
      "Cumulative Time: 927.8091s\n",
      "Loss: 0.0191\n",
      "========================================\n",
      "Batch:  2640\n",
      "Cumulative Time: 941.9021s\n",
      "Loss: 0.0191\n",
      "========================================\n",
      "Batch:  2680\n",
      "Cumulative Time: 955.9073s\n",
      "Loss: 0.0191\n",
      "========================================\n",
      "Batch:  2720\n",
      "Cumulative Time: 970.0183s\n",
      "Loss: 0.0191\n",
      "========================================\n",
      "Batch:  2760\n",
      "Cumulative Time: 984.2564s\n",
      "Loss: 0.0191\n",
      "========================================\n",
      "Batch:  2800\n",
      "Cumulative Time: 998.3240s\n",
      "Loss: 0.0191\n",
      "========================================\n",
      "Batch:  2840\n",
      "Cumulative Time: 1012.5917s\n",
      "Loss: 0.0191\n",
      "========================================\n",
      "Batch:  2880\n",
      "Cumulative Time: 1027.0617s\n",
      "Loss: 0.0191\n",
      "========================================\n",
      "Batch:  2920\n",
      "Cumulative Time: 1041.4782s\n",
      "Loss: 0.0191\n",
      "========================================\n",
      "Batch:  2960\n",
      "Cumulative Time: 1055.8382s\n",
      "Loss: 0.0191\n",
      "========================================\n",
      "Batch:  3000\n",
      "Cumulative Time: 1070.2715s\n",
      "Loss: 0.0191\n",
      "========================================\n",
      "Batch:  3040\n",
      "Cumulative Time: 1084.7328s\n",
      "Loss: 0.0191\n",
      "========================================\n",
      "Batch:  3080\n",
      "Cumulative Time: 1099.1479s\n",
      "Loss: 0.0191\n",
      "========================================\n",
      "Batch:  3120\n",
      "Cumulative Time: 1113.5918s\n",
      "Loss: 0.0191\n",
      "========================================\n",
      "Batch:  3160\n",
      "Cumulative Time: 1128.0108s\n",
      "Loss: 0.0191\n",
      "========================================\n",
      "Batch:  3200\n",
      "Cumulative Time: 1142.4748s\n",
      "Loss: 0.0191\n",
      "========================================\n",
      "Batch:  3240\n",
      "Cumulative Time: 1156.9126s\n",
      "Loss: 0.0191\n",
      "========================================\n",
      "Batch:  3280\n",
      "Cumulative Time: 1171.3816s\n",
      "Loss: 0.0191\n",
      "========================================\n",
      "Batch:  3320\n",
      "Cumulative Time: 1185.7836s\n",
      "Loss: 0.0191\n",
      "========================================\n",
      "Batch:  3360\n",
      "Cumulative Time: 1200.2207s\n",
      "Loss: 0.0191\n",
      "========================================\n",
      "Batch:  3400\n",
      "Cumulative Time: 1214.7206s\n",
      "Loss: 0.0191\n",
      "========================================\n",
      "Batch:  3440\n",
      "Cumulative Time: 1229.2169s\n",
      "Loss: 0.0190\n",
      "========================================\n",
      "Batch:  3480\n",
      "Cumulative Time: 1243.6839s\n",
      "Loss: 0.0190\n",
      "========================================\n",
      "Batch:  3520\n",
      "Cumulative Time: 1258.0578s\n",
      "Loss: 0.0190\n",
      "========================================\n",
      "Batch:  3560\n",
      "Cumulative Time: 1272.5279s\n",
      "Loss: 0.0190\n",
      "========================================\n",
      "Batch:  3600\n",
      "Cumulative Time: 1287.0013s\n",
      "Loss: 0.0190\n",
      "========================================\n",
      "Batch:  3640\n",
      "Cumulative Time: 1301.4249s\n",
      "Loss: 0.0190\n",
      "========================================\n",
      "Batch:  3680\n",
      "Cumulative Time: 1315.9342s\n",
      "Loss: 0.0190\n",
      "========================================\n",
      "Batch:  3720\n",
      "Cumulative Time: 1330.4046s\n",
      "Loss: 0.0190\n",
      "========================================\n",
      "Batch:  3760\n",
      "Cumulative Time: 1344.8684s\n",
      "Loss: 0.0190\n",
      "========================================\n",
      "Batch:  3800\n",
      "Cumulative Time: 1359.2019s\n",
      "Loss: 0.0190\n",
      "========================================\n",
      "Batch:  3840\n",
      "Cumulative Time: 1373.6208s\n",
      "Loss: 0.0190\n",
      "========================================\n",
      "Batch:  3880\n",
      "Cumulative Time: 1388.0429s\n",
      "Loss: 0.0190\n",
      "========================================\n",
      "Batch:  3920\n",
      "Cumulative Time: 1402.5030s\n",
      "Loss: 0.0190\n",
      "========================================\n",
      "Batch:  3960\n",
      "Cumulative Time: 1416.9447s\n",
      "Loss: 0.0190\n",
      "========================================\n",
      "Batch:  4000\n",
      "Cumulative Time: 1431.0923s\n",
      "Loss: 0.0190\n",
      "========================================\n",
      "Batch:  4040\n",
      "Cumulative Time: 1445.0872s\n",
      "Loss: 0.0190\n",
      "========================================\n",
      "Batch:  4080\n",
      "Cumulative Time: 1459.1551s\n",
      "Loss: 0.0190\n",
      "========================================\n",
      "Batch:  4120\n",
      "Cumulative Time: 1473.1478s\n",
      "Loss: 0.0190\n",
      "========================================\n",
      "Batch:  4160\n",
      "Cumulative Time: 1487.4894s\n",
      "Loss: 0.0190\n",
      "========================================\n",
      "Batch:  4200\n",
      "Cumulative Time: 1501.4747s\n",
      "Loss: 0.0190\n",
      "========================================\n",
      "Batch:  4240\n",
      "Cumulative Time: 1515.7649s\n",
      "Loss: 0.0190\n",
      "========================================\n",
      "Batch:  4280\n",
      "Cumulative Time: 1529.7942s\n",
      "Loss: 0.0190\n",
      "========================================\n",
      "Batch:  4320\n",
      "Cumulative Time: 1543.9261s\n",
      "Loss: 0.0190\n",
      "========================================\n",
      "Batch:  4360\n",
      "Cumulative Time: 1557.9585s\n",
      "Loss: 0.0190\n",
      "========================================\n",
      "Batch:  4400\n",
      "Cumulative Time: 1572.0526s\n",
      "Loss: 0.0190\n",
      "========================================\n",
      "Batch:  4440\n",
      "Cumulative Time: 1586.4316s\n",
      "Loss: 0.0190\n",
      "========================================\n",
      "Batch:  4480\n",
      "Cumulative Time: 1600.7279s\n",
      "Loss: 0.0190\n",
      "========================================\n",
      "Batch:  4520\n",
      "Cumulative Time: 1614.8325s\n",
      "Loss: 0.0190\n",
      "========================================\n",
      "Batch:  4560\n",
      "Cumulative Time: 1629.0472s\n",
      "Loss: 0.0190\n",
      "========================================\n",
      "Batch:  4600\n",
      "Cumulative Time: 1643.3837s\n",
      "Loss: 0.0190\n",
      "========================================\n",
      "Batch:  4640\n",
      "Cumulative Time: 1657.4671s\n",
      "Loss: 0.0190\n",
      "========================================\n",
      "Batch:  4680\n",
      "Cumulative Time: 1671.6799s\n",
      "Loss: 0.0190\n",
      "========================================\n",
      "Batch:  4720\n",
      "Cumulative Time: 1685.8374s\n",
      "Loss: 0.0190\n",
      "========================================\n",
      "Batch:  4760\n",
      "Cumulative Time: 1700.1215s\n",
      "Loss: 0.0190\n",
      "========================================\n",
      "Batch:  4800\n",
      "Cumulative Time: 1714.2231s\n",
      "Loss: 0.0190\n",
      "========================================\n",
      "Batch:  4840\n",
      "Cumulative Time: 1728.2264s\n",
      "Loss: 0.0190\n",
      "========================================\n",
      "Batch:  4880\n",
      "Cumulative Time: 1742.6626s\n",
      "Loss: 0.0190\n",
      "========================================\n",
      "Batch:  4920\n",
      "Cumulative Time: 1756.7940s\n",
      "Loss: 0.0190\n",
      "========================================\n",
      "Batch:  4960\n",
      "Cumulative Time: 1770.8111s\n",
      "Loss: 0.0190\n",
      "========================================\n",
      "Batch:  5000\n",
      "Cumulative Time: 1784.8726s\n",
      "Loss: 0.0190\n",
      "========================================\n",
      "Batch:  5040\n",
      "Cumulative Time: 1799.1501s\n",
      "Loss: 0.0190\n",
      "========================================\n",
      "Batch:  5080\n",
      "Cumulative Time: 1813.3260s\n",
      "Loss: 0.0190\n",
      "========================================\n",
      "Batch:  5120\n",
      "Cumulative Time: 1827.3976s\n",
      "Loss: 0.0190\n",
      "========================================\n",
      "Batch:  5160\n",
      "Cumulative Time: 1841.5597s\n",
      "Loss: 0.0190\n",
      "========================================\n",
      "Batch:  5200\n",
      "Cumulative Time: 1855.6071s\n",
      "Loss: 0.0190\n",
      "========================================\n",
      "Batch:  5240\n",
      "Cumulative Time: 1869.7104s\n",
      "Loss: 0.0190\n",
      "========================================\n",
      "Batch:  5280\n",
      "Cumulative Time: 1883.8432s\n",
      "Loss: 0.0190\n",
      "========================================\n",
      "Batch:  5320\n",
      "Cumulative Time: 1897.9099s\n",
      "Loss: 0.0190\n",
      "========================================\n",
      "Batch:  5360\n",
      "Cumulative Time: 1912.1400s\n",
      "Loss: 0.0190\n",
      "========================================\n",
      "Batch:  5400\n",
      "Cumulative Time: 1926.3879s\n",
      "Loss: 0.0190\n",
      "========================================\n",
      "Batch:  5440\n",
      "Cumulative Time: 1940.6233s\n",
      "Loss: 0.0190\n",
      "========================================\n",
      "Batch:  5480\n",
      "Cumulative Time: 1954.7459s\n",
      "Loss: 0.0189\n",
      "========================================\n",
      "Batch:  5520\n",
      "Cumulative Time: 1968.7417s\n",
      "Loss: 0.0189\n",
      "========================================\n",
      "Batch:  5560\n",
      "Cumulative Time: 1982.9447s\n",
      "Loss: 0.0189\n",
      "========================================\n",
      "Batch:  5600\n",
      "Cumulative Time: 1997.1589s\n",
      "Loss: 0.0189\n",
      "========================================\n",
      "Batch:  5640\n",
      "Cumulative Time: 2011.3393s\n",
      "Loss: 0.0189\n",
      "========================================\n",
      "Batch:  5680\n",
      "Cumulative Time: 2025.4367s\n",
      "Loss: 0.0189\n",
      "========================================\n",
      "Batch:  5720\n",
      "Cumulative Time: 2039.5027s\n",
      "Loss: 0.0189\n",
      "========================================\n",
      "Batch:  5760\n",
      "Cumulative Time: 2053.5272s\n",
      "Loss: 0.0189\n",
      "========================================\n",
      "Batch:  5800\n",
      "Cumulative Time: 2067.7110s\n",
      "Loss: 0.0189\n",
      "========================================\n",
      "Batch:  5840\n",
      "Cumulative Time: 2081.8384s\n",
      "Loss: 0.0189\n",
      "========================================\n",
      "Batch:  5880\n",
      "Cumulative Time: 2095.9003s\n",
      "Loss: 0.0189\n",
      "========================================\n",
      "Batch:  5920\n",
      "Cumulative Time: 2110.3322s\n",
      "Loss: 0.0189\n",
      "========================================\n",
      "Batch:  5960\n",
      "Cumulative Time: 2124.6874s\n",
      "Loss: 0.0189\n",
      "========================================\n",
      "Batch:  6000\n",
      "Cumulative Time: 2139.0666s\n",
      "Loss: 0.0189\n",
      "========================================\n",
      "Batch:  6040\n",
      "Cumulative Time: 2153.5190s\n",
      "Loss: 0.0189\n",
      "========================================\n",
      "Batch:  6080\n",
      "Cumulative Time: 2167.9756s\n",
      "Loss: 0.0189\n",
      "========================================\n",
      "Batch:  6120\n",
      "Cumulative Time: 2182.4783s\n",
      "Loss: 0.0189\n",
      "========================================\n",
      "Batch:  6160\n",
      "Cumulative Time: 2196.9446s\n",
      "Loss: 0.0189\n",
      "========================================\n",
      "Batch:  6200\n",
      "Cumulative Time: 2211.0417s\n",
      "Loss: 0.0189\n",
      "========================================\n",
      "Batch:  6240\n",
      "Cumulative Time: 2225.1549s\n",
      "Loss: 0.0189\n",
      "========================================\n",
      "Batch:  6280\n",
      "Cumulative Time: 2239.5881s\n",
      "Loss: 0.0189\n",
      "========================================\n",
      "Batch:  6320\n",
      "Cumulative Time: 2253.8121s\n",
      "Loss: 0.0189\n",
      "========================================\n",
      "Batch:  6360\n",
      "Cumulative Time: 2268.2859s\n",
      "Loss: 0.0189\n",
      "========================================\n",
      "Batch:  6400\n",
      "Cumulative Time: 2282.6659s\n",
      "Loss: 0.0189\n",
      "========================================\n",
      "Batch:  6440\n",
      "Cumulative Time: 2297.1205s\n",
      "Loss: 0.0189\n",
      "========================================\n",
      "Batch:  6480\n",
      "Cumulative Time: 2311.4855s\n",
      "Loss: 0.0189\n",
      "========================================\n",
      "Batch:  6520\n",
      "Cumulative Time: 2325.8705s\n",
      "Loss: 0.0189\n",
      "========================================\n",
      "Batch:  6560\n",
      "Cumulative Time: 2340.3181s\n",
      "Loss: 0.0189\n",
      "========================================\n",
      "Batch:  6600\n",
      "Cumulative Time: 2354.7942s\n",
      "Loss: 0.0189\n",
      "========================================\n",
      "Batch:  6640\n",
      "Cumulative Time: 2369.2278s\n",
      "Loss: 0.0189\n",
      "========================================\n",
      "Batch:  6680\n",
      "Cumulative Time: 2383.6751s\n",
      "Loss: 0.0189\n",
      "========================================\n",
      "Batch:  6720\n",
      "Cumulative Time: 2398.1607s\n",
      "Loss: 0.0189\n",
      "========================================\n",
      "Batch:  6760\n",
      "Cumulative Time: 2412.5442s\n",
      "Loss: 0.0189\n",
      "========================================\n",
      "Batch:  6800\n",
      "Cumulative Time: 2426.9587s\n",
      "Loss: 0.0189\n",
      "========================================\n",
      "Batch:  6840\n",
      "Cumulative Time: 2441.3804s\n",
      "Loss: 0.0189\n",
      "========================================\n",
      "Batch:  6880\n",
      "Cumulative Time: 2455.7604s\n",
      "Loss: 0.0189\n",
      "========================================\n",
      "Batch:  6920\n",
      "Cumulative Time: 2470.1556s\n",
      "Loss: 0.0189\n",
      "========================================\n",
      "Batch:  6960\n",
      "Cumulative Time: 2484.5643s\n",
      "Loss: 0.0189\n",
      "========================================\n",
      "Batch:  7000\n",
      "Cumulative Time: 2499.0551s\n",
      "Loss: 0.0189\n",
      "========================================\n",
      "Batch:  7040\n",
      "Cumulative Time: 2513.5081s\n",
      "Loss: 0.0189\n",
      "========================================\n",
      "Batch:  7080\n",
      "Cumulative Time: 2527.9103s\n",
      "Loss: 0.0189\n",
      "========================================\n",
      "Batch:  7120\n",
      "Cumulative Time: 2542.3321s\n",
      "Loss: 0.0189\n",
      "========================================\n",
      "Batch:  7160\n",
      "Cumulative Time: 2556.7288s\n",
      "Loss: 0.0189\n",
      "========================================\n",
      "Batch:  7200\n",
      "Cumulative Time: 2571.2278s\n",
      "Loss: 0.0189\n",
      "========================================\n",
      "Batch:  7240\n",
      "Cumulative Time: 2585.7277s\n",
      "Loss: 0.0189\n",
      "========================================\n",
      "Batch:  7280\n",
      "Cumulative Time: 2600.0153s\n",
      "Loss: 0.0189\n",
      "========================================\n",
      "Batch:  7320\n",
      "Cumulative Time: 2614.1808s\n",
      "Loss: 0.0189\n",
      "========================================\n",
      "Batch:  7360\n",
      "Cumulative Time: 2628.4422s\n",
      "Loss: 0.0189\n",
      "========================================\n",
      "Batch:  7400\n",
      "Cumulative Time: 2642.6627s\n",
      "Loss: 0.0189\n",
      "========================================\n",
      "Batch:  7440\n",
      "Cumulative Time: 2656.9796s\n",
      "Loss: 0.0189\n",
      "========================================\n",
      "Batch:  7480\n",
      "Cumulative Time: 2670.9799s\n",
      "Loss: 0.0189\n",
      "========================================\n",
      "Batch:  7520\n",
      "Cumulative Time: 2685.0850s\n",
      "Loss: 0.0189\n",
      "========================================\n",
      "Batch:  7560\n",
      "Cumulative Time: 2699.2474s\n",
      "Loss: 0.0189\n",
      "========================================\n",
      "Batch:  7600\n",
      "Cumulative Time: 2713.2381s\n",
      "Loss: 0.0189\n",
      "========================================\n",
      "Batch:  7640\n",
      "Cumulative Time: 2727.3546s\n",
      "Loss: 0.0189\n",
      "========================================\n",
      "Batch:  7680\n",
      "Cumulative Time: 2741.4003s\n",
      "Loss: 0.0189\n",
      "========================================\n",
      "Batch:  7720\n",
      "Cumulative Time: 2755.4230s\n",
      "Loss: 0.0189\n",
      "========================================\n",
      "Batch:  7760\n",
      "Cumulative Time: 2769.5105s\n",
      "Loss: 0.0189\n",
      "========================================\n",
      "Batch:  7800\n",
      "Cumulative Time: 2783.5180s\n",
      "Loss: 0.0189\n",
      "========================================\n",
      "Batch:  7840\n",
      "Cumulative Time: 2798.0029s\n",
      "Loss: 0.0189\n",
      "========================================\n",
      "Batch:  7880\n",
      "Cumulative Time: 2812.4387s\n",
      "Loss: 0.0189\n",
      "========================================\n",
      "Batch:  7920\n",
      "Cumulative Time: 2826.8970s\n",
      "Loss: 0.0189\n",
      "========================================\n",
      "Batch:  7960\n",
      "Cumulative Time: 2841.3334s\n",
      "Loss: 0.0189\n",
      "========================================\n",
      "Batch:  8000\n",
      "Cumulative Time: 2855.8061s\n",
      "Loss: 0.0189\n",
      "========================================\n",
      "Batch:  8040\n",
      "Cumulative Time: 2870.2795s\n",
      "Loss: 0.0189\n",
      "========================================\n",
      "Batch:  8080\n",
      "Cumulative Time: 2884.5933s\n",
      "Loss: 0.0189\n",
      "========================================\n",
      "Batch:  8120\n",
      "Cumulative Time: 2898.9950s\n",
      "Loss: 0.0189\n",
      "========================================\n",
      "Batch:  8160\n",
      "Cumulative Time: 2913.1348s\n",
      "Loss: 0.0189\n",
      "========================================\n",
      "Batch:  8200\n",
      "Cumulative Time: 2927.3433s\n",
      "Loss: 0.0188\n",
      "========================================\n",
      "Batch:  8240\n",
      "Cumulative Time: 2941.6588s\n",
      "Loss: 0.0189\n",
      "========================================\n",
      "Batch:  8280\n",
      "Cumulative Time: 2956.0010s\n",
      "Loss: 0.0188\n",
      "========================================\n",
      "Batch:  8320\n",
      "Cumulative Time: 2970.4092s\n",
      "Loss: 0.0188\n",
      "========================================\n",
      "Batch:  8360\n",
      "Cumulative Time: 2984.7123s\n",
      "Loss: 0.0188\n",
      "========================================\n",
      "Batch:  8400\n",
      "Cumulative Time: 2999.0809s\n",
      "Loss: 0.0188\n",
      "========================================\n",
      "Batch:  8440\n",
      "Cumulative Time: 3013.5218s\n",
      "Loss: 0.0188\n",
      "========================================\n",
      "Batch:  8480\n",
      "Cumulative Time: 3027.7467s\n",
      "Loss: 0.0188\n",
      "========================================\n",
      "Batch:  8520\n",
      "Cumulative Time: 3042.2492s\n",
      "Loss: 0.0188\n",
      "========================================\n",
      "Batch:  8560\n",
      "Cumulative Time: 3056.7095s\n",
      "Loss: 0.0188\n",
      "========================================\n",
      "Batch:  8600\n",
      "Cumulative Time: 3070.7111s\n",
      "Loss: 0.0188\n",
      "========================================\n",
      "Batch:  8640\n",
      "Cumulative Time: 3084.8204s\n",
      "Loss: 0.0188\n",
      "========================================\n",
      "Batch:  8680\n",
      "Cumulative Time: 3099.3346s\n",
      "Loss: 0.0188\n",
      "========================================\n",
      "Batch:  8720\n",
      "Cumulative Time: 3113.5221s\n",
      "Loss: 0.0188\n",
      "========================================\n",
      "Batch:  8760\n",
      "Cumulative Time: 3127.8449s\n",
      "Loss: 0.0188\n",
      "========================================\n",
      "Batch:  8800\n",
      "Cumulative Time: 3141.9727s\n",
      "Loss: 0.0188\n",
      "========================================\n",
      "Batch:  8840\n",
      "Cumulative Time: 3155.9751s\n",
      "Loss: 0.0188\n",
      "========================================\n",
      "Batch:  8880\n",
      "Cumulative Time: 3170.2650s\n",
      "Loss: 0.0188\n",
      "========================================\n",
      "Batch:  8920\n",
      "Cumulative Time: 3184.5418s\n",
      "Loss: 0.0188\n",
      "========================================\n",
      "Batch:  8960\n",
      "Cumulative Time: 3198.6484s\n",
      "Loss: 0.0188\n",
      "========================================\n",
      "Batch:  9000\n",
      "Cumulative Time: 3212.6759s\n",
      "Loss: 0.0188\n",
      "========================================\n",
      "Batch:  9040\n",
      "Cumulative Time: 3226.8368s\n",
      "Loss: 0.0188\n",
      "========================================\n",
      "Batch:  9080\n",
      "Cumulative Time: 3241.0927s\n",
      "Loss: 0.0188\n",
      "========================================\n",
      "Batch:  9120\n",
      "Cumulative Time: 3255.5101s\n",
      "Loss: 0.0188\n",
      "========================================\n",
      "Batch:  9160\n",
      "Cumulative Time: 3269.4899s\n",
      "Loss: 0.0188\n",
      "========================================\n",
      "Batch:  9200\n",
      "Cumulative Time: 3283.5848s\n",
      "Loss: 0.0188\n",
      "========================================\n",
      "Batch:  9240\n",
      "Cumulative Time: 3297.8194s\n",
      "Loss: 0.0188\n",
      "========================================\n",
      "Batch:  9280\n",
      "Cumulative Time: 3312.0152s\n",
      "Loss: 0.0188\n",
      "========================================\n",
      "Batch:  9320\n",
      "Cumulative Time: 3326.3420s\n",
      "Loss: 0.0188\n",
      "========================================\n",
      "Batch:  9360\n",
      "Cumulative Time: 3340.9453s\n",
      "Loss: 0.0188\n",
      "========================================\n",
      "Batch:  9400\n",
      "Cumulative Time: 3355.2321s\n",
      "Loss: 0.0188\n",
      "========================================\n",
      "Batch:  9440\n",
      "Cumulative Time: 3369.3934s\n",
      "Loss: 0.0188\n",
      "========================================\n",
      "Batch:  9480\n",
      "Cumulative Time: 3383.8288s\n",
      "Loss: 0.0188\n",
      "========================================\n",
      "Batch:  9520\n",
      "Cumulative Time: 3398.2476s\n",
      "Loss: 0.0188\n",
      "========================================\n",
      "Batch:  9560\n",
      "Cumulative Time: 3412.5073s\n",
      "Loss: 0.0188\n",
      "========================================\n",
      "Batch:  9600\n",
      "Cumulative Time: 3426.8910s\n",
      "Loss: 0.0188\n",
      "========================================\n",
      "Batch:  9640\n",
      "Cumulative Time: 3441.1642s\n",
      "Loss: 0.0188\n",
      "========================================\n",
      "Batch:  9680\n",
      "Cumulative Time: 3455.4929s\n",
      "Loss: 0.0188\n",
      "========================================\n",
      "Batch:  9720\n",
      "Cumulative Time: 3469.7509s\n",
      "Loss: 0.0188\n",
      "========================================\n",
      "Batch:  9760\n",
      "Cumulative Time: 3484.0210s\n",
      "Loss: 0.0188\n",
      "========================================\n",
      "Batch:  9800\n",
      "Cumulative Time: 3498.2055s\n",
      "Loss: 0.0188\n",
      "========================================\n",
      "Batch:  9840\n",
      "Cumulative Time: 3512.3963s\n",
      "Loss: 0.0188\n",
      "========================================\n",
      "Batch:  9880\n",
      "Cumulative Time: 3526.7589s\n",
      "Loss: 0.0188\n",
      "========================================\n",
      "Batch:  9920\n",
      "Cumulative Time: 3541.1076s\n",
      "Loss: 0.0188\n",
      "========================================\n",
      "Batch:  9960\n",
      "Cumulative Time: 3555.3260s\n",
      "Loss: 0.0188\n",
      "========================================\n",
      "Batch:  10000\n",
      "Cumulative Time: 3569.5214s\n",
      "Loss: 0.0188\n",
      "========================================\n",
      "Batch:  10040\n",
      "Cumulative Time: 3583.9845s\n",
      "Loss: 0.0188\n",
      "========================================\n",
      "Batch:  10080\n",
      "Cumulative Time: 3598.2486s\n",
      "Loss: 0.0188\n",
      "========================================\n",
      "Batch:  10120\n",
      "Cumulative Time: 3612.5161s\n",
      "Loss: 0.0188\n",
      "========================================\n",
      "Batch:  10160\n",
      "Cumulative Time: 3626.6020s\n",
      "Loss: 0.0188\n",
      "========================================\n",
      "Batch:  10200\n",
      "Cumulative Time: 3641.0615s\n",
      "Loss: 0.0188\n",
      "========================================\n",
      "Batch:  10240\n",
      "Cumulative Time: 3655.3237s\n",
      "Loss: 0.0188\n",
      "========================================\n",
      "Batch:  10280\n",
      "Cumulative Time: 3669.5429s\n",
      "Loss: 0.0188\n",
      "========================================\n",
      "Batch:  10320\n",
      "Cumulative Time: 3683.7340s\n",
      "Loss: 0.0188\n",
      "========================================\n",
      "Batch:  10360\n",
      "Cumulative Time: 3697.8164s\n",
      "Loss: 0.0188\n",
      "========================================\n",
      "Batch:  10400\n",
      "Cumulative Time: 3711.9554s\n",
      "Loss: 0.0188\n",
      "========================================\n",
      "Batch:  10440\n",
      "Cumulative Time: 3726.0253s\n",
      "Loss: 0.0188\n",
      "========================================\n",
      "Batch:  10480\n",
      "Cumulative Time: 3740.3417s\n",
      "Loss: 0.0188\n",
      "========================================\n",
      "Batch:  10520\n",
      "Cumulative Time: 3754.4777s\n",
      "Loss: 0.0188\n",
      "========================================\n",
      "Batch:  10560\n",
      "Cumulative Time: 3768.6613s\n",
      "Loss: 0.0188\n",
      "========================================\n",
      "Batch:  10600\n",
      "Cumulative Time: 3782.9416s\n",
      "Loss: 0.0188\n",
      "========================================\n",
      "Batch:  10640\n",
      "Cumulative Time: 3797.0085s\n",
      "Loss: 0.0188\n",
      "========================================\n",
      "Batch:  10680\n",
      "Cumulative Time: 3811.1381s\n",
      "Loss: 0.0188\n",
      "========================================\n",
      "Batch:  10720\n",
      "Cumulative Time: 3825.2707s\n",
      "Loss: 0.0188\n",
      "========================================\n",
      "Batch:  10760\n",
      "Cumulative Time: 3839.2888s\n",
      "Loss: 0.0188\n",
      "========================================\n",
      "Batch:  10800\n",
      "Cumulative Time: 3853.3435s\n",
      "Loss: 0.0188\n",
      "========================================\n",
      "Batch:  10840\n",
      "Cumulative Time: 3867.3532s\n",
      "Loss: 0.0188\n",
      "========================================\n",
      "Batch:  10880\n",
      "Cumulative Time: 3881.4759s\n",
      "Loss: 0.0188\n",
      "========================================\n",
      "Batch:  10920\n",
      "Cumulative Time: 3895.5299s\n",
      "Loss: 0.0188\n",
      "========================================\n",
      "Batch:  10960\n",
      "Cumulative Time: 3909.5489s\n",
      "Loss: 0.0188\n",
      "========================================\n",
      "Batch:  11000\n",
      "Cumulative Time: 3923.6145s\n",
      "Loss: 0.0188\n",
      "========================================\n",
      "Batch:  11040\n",
      "Cumulative Time: 3937.6110s\n",
      "Loss: 0.0188\n",
      "========================================\n",
      "Batch:  11080\n",
      "Cumulative Time: 3952.0945s\n",
      "Loss: 0.0188\n",
      "========================================\n",
      "Batch:  11120\n",
      "Cumulative Time: 3966.5729s\n",
      "Loss: 0.0188\n",
      "========================================\n",
      "Batch:  11160\n",
      "Cumulative Time: 3981.0268s\n",
      "Loss: 0.0188\n",
      "========================================\n",
      "Batch:  11200\n",
      "Cumulative Time: 3995.4848s\n",
      "Loss: 0.0188\n",
      "========================================\n",
      "Batch:  11240\n",
      "Cumulative Time: 4009.8923s\n",
      "Loss: 0.0188\n",
      "========================================\n",
      "Batch:  11280\n",
      "Cumulative Time: 4024.3643s\n",
      "Loss: 0.0188\n",
      "========================================\n",
      "Batch:  11320\n",
      "Cumulative Time: 4038.7398s\n",
      "Loss: 0.0188\n",
      "========================================\n",
      "Batch:  11360\n",
      "Cumulative Time: 4053.0983s\n",
      "Loss: 0.0188\n",
      "========================================\n",
      "Batch:  11400\n",
      "Cumulative Time: 4067.3606s\n",
      "Loss: 0.0188\n",
      "========================================\n",
      "Batch:  11440\n",
      "Cumulative Time: 4081.7609s\n",
      "Loss: 0.0188\n",
      "========================================\n",
      "Batch:  11480\n",
      "Cumulative Time: 4096.0972s\n",
      "Loss: 0.0188\n",
      "========================================\n",
      "Batch:  11520\n",
      "Cumulative Time: 4110.2318s\n",
      "Loss: 0.0188\n",
      "========================================\n",
      "Batch:  11560\n",
      "Cumulative Time: 4124.2492s\n",
      "Loss: 0.0188\n",
      "========================================\n",
      "Batch:  11600\n",
      "Cumulative Time: 4138.5225s\n",
      "Loss: 0.0188\n",
      "========================================\n",
      "Batch:  11640\n",
      "Cumulative Time: 4153.3129s\n",
      "Loss: 0.0188\n",
      "========================================\n",
      "Batch:  11680\n",
      "Cumulative Time: 4167.5917s\n",
      "Loss: 0.0188\n",
      "========================================\n",
      "Batch:  11720\n",
      "Cumulative Time: 4182.5768s\n",
      "Loss: 0.0188\n",
      "========================================\n",
      "Batch:  11760\n",
      "Cumulative Time: 4196.9777s\n",
      "Loss: 0.0188\n",
      "========================================\n",
      "Batch:  11800\n",
      "Cumulative Time: 4211.9174s\n",
      "Loss: 0.0188\n",
      "========================================\n",
      "Batch:  11840\n",
      "Cumulative Time: 4226.3538s\n",
      "Loss: 0.0188\n",
      "========================================\n",
      "Batch:  11880\n",
      "Cumulative Time: 4240.8025s\n",
      "Loss: 0.0188\n",
      "========================================\n",
      "Batch:  11920\n",
      "Cumulative Time: 4255.1976s\n",
      "Loss: 0.0188\n",
      "========================================\n",
      "Batch:  11960\n",
      "Cumulative Time: 4269.6093s\n",
      "Loss: 0.0188\n",
      "========================================\n",
      "Batch:  12000\n",
      "Cumulative Time: 4284.0760s\n",
      "Loss: 0.0188\n",
      "========================================\n",
      "Batch:  12040\n",
      "Cumulative Time: 4299.0007s\n",
      "Loss: 0.0188\n",
      "========================================\n",
      "Batch:  12080\n",
      "Cumulative Time: 4313.3907s\n",
      "Loss: 0.0188\n",
      "========================================\n",
      "Batch:  12120\n",
      "Cumulative Time: 4327.7594s\n",
      "Loss: 0.0188\n",
      "========================================\n",
      "Batch:  12160\n",
      "Cumulative Time: 4342.2001s\n",
      "Loss: 0.0188\n",
      "========================================\n",
      "Batch:  12200\n",
      "Cumulative Time: 4356.5557s\n",
      "Loss: 0.0188\n",
      "========================================\n",
      "Batch:  12240\n",
      "Cumulative Time: 4370.9514s\n",
      "Loss: 0.0188\n",
      "========================================\n",
      "Batch:  12280\n",
      "Cumulative Time: 4385.4403s\n",
      "Loss: 0.0188\n",
      "========================================\n",
      "Batch:  12320\n",
      "Cumulative Time: 4399.7897s\n",
      "Loss: 0.0188\n",
      "========================================\n",
      "Batch:  12360\n",
      "Cumulative Time: 4414.6474s\n",
      "Loss: 0.0188\n",
      "========================================\n",
      "Batch:  12400\n",
      "Cumulative Time: 4436.2088s\n",
      "Loss: 0.0188\n",
      "========================================\n",
      "Batch:  12440\n",
      "Cumulative Time: 4458.0197s\n",
      "Loss: 0.0187\n",
      "========================================\n",
      "Batch:  12480\n",
      "Cumulative Time: 4479.8502s\n",
      "Loss: 0.0187\n",
      "========================================\n",
      "Batch:  12520\n",
      "Cumulative Time: 4501.8317s\n",
      "Loss: 0.0187\n",
      "========================================\n",
      "Batch:  12560\n",
      "Cumulative Time: 4523.6340s\n",
      "Loss: 0.0187\n",
      "========================================\n",
      "Batch:  12600\n",
      "Cumulative Time: 4545.4342s\n",
      "Loss: 0.0187\n",
      "========================================\n",
      "Batch:  12640\n",
      "Cumulative Time: 4567.4938s\n",
      "Loss: 0.0187\n",
      "========================================\n",
      "Batch:  12680\n",
      "Cumulative Time: 4589.9157s\n",
      "Loss: 0.0187\n",
      "========================================\n",
      "Batch:  12720\n",
      "Cumulative Time: 4612.1771s\n",
      "Loss: 0.0187\n",
      "========================================\n",
      "Batch:  12760\n",
      "Cumulative Time: 4634.0093s\n",
      "Loss: 0.0187\n",
      "========================================\n",
      "Batch:  12800\n",
      "Cumulative Time: 4655.7562s\n",
      "Loss: 0.0187\n",
      "========================================\n",
      "Batch:  12840\n",
      "Cumulative Time: 4677.6795s\n",
      "Loss: 0.0187\n",
      "========================================\n",
      "Batch:  12880\n",
      "Cumulative Time: 4699.6456s\n",
      "Loss: 0.0187\n",
      "========================================\n",
      "Batch:  12920\n",
      "Cumulative Time: 4716.9485s\n",
      "Loss: 0.0187\n",
      "========================================\n",
      "Batch:  12960\n",
      "Cumulative Time: 4731.2349s\n",
      "Loss: 0.0187\n",
      "========================================\n",
      "Batch:  13000\n",
      "Cumulative Time: 4745.6602s\n",
      "Loss: 0.0187\n",
      "========================================\n",
      "Batch:  13040\n",
      "Cumulative Time: 4760.1098s\n",
      "Loss: 0.0187\n",
      "========================================\n",
      "Batch:  13080\n",
      "Cumulative Time: 4774.5740s\n",
      "Loss: 0.0187\n",
      "========================================\n",
      "Batch:  13120\n",
      "Cumulative Time: 4789.0364s\n",
      "Loss: 0.0187\n",
      "========================================\n",
      "Batch:  13160\n",
      "Cumulative Time: 4803.5156s\n",
      "Loss: 0.0187\n",
      "========================================\n",
      "Batch:  13200\n",
      "Cumulative Time: 4817.7167s\n",
      "Loss: 0.0187\n",
      "========================================\n",
      "Batch:  13240\n",
      "Cumulative Time: 4832.0842s\n",
      "Loss: 0.0187\n",
      "========================================\n",
      "Batch:  13280\n",
      "Cumulative Time: 4846.5155s\n",
      "Loss: 0.0187\n",
      "========================================\n",
      "Batch:  13320\n",
      "Cumulative Time: 4860.9817s\n",
      "Loss: 0.0187\n",
      "========================================\n",
      "Batch:  13360\n",
      "Cumulative Time: 4875.4305s\n",
      "Loss: 0.0187\n",
      "========================================\n",
      "Batch:  13400\n",
      "Cumulative Time: 4889.8691s\n",
      "Loss: 0.0187\n",
      "========================================\n",
      "Batch:  13440\n",
      "Cumulative Time: 4904.0895s\n",
      "Loss: 0.0187\n",
      "========================================\n",
      "Batch:  13480\n",
      "Cumulative Time: 4918.3740s\n",
      "Loss: 0.0187\n",
      "========================================\n",
      "Batch:  13520\n",
      "Cumulative Time: 4932.7359s\n",
      "Loss: 0.0187\n",
      "========================================\n",
      "Batch:  13560\n",
      "Cumulative Time: 4946.9414s\n",
      "Loss: 0.0187\n",
      "========================================\n",
      "Batch:  13600\n",
      "Cumulative Time: 4961.0597s\n",
      "Loss: 0.0187\n",
      "========================================\n",
      "Batch:  13640\n",
      "Cumulative Time: 4975.1450s\n",
      "Loss: 0.0187\n",
      "========================================\n",
      "Batch:  13680\n",
      "Cumulative Time: 4989.5132s\n",
      "Loss: 0.0187\n",
      "========================================\n",
      "Batch:  13720\n",
      "Cumulative Time: 5003.5816s\n",
      "Loss: 0.0187\n",
      "========================================\n",
      "Batch:  13760\n",
      "Cumulative Time: 5017.7234s\n",
      "Loss: 0.0187\n",
      "========================================\n",
      "Batch:  13800\n",
      "Cumulative Time: 5032.1188s\n",
      "Loss: 0.0187\n",
      "========================================\n",
      "Batch:  13840\n",
      "Cumulative Time: 5051.7053s\n",
      "Loss: 0.0187\n",
      "========================================\n",
      "Batch:  13880\n",
      "Cumulative Time: 5073.6211s\n",
      "Loss: 0.0187\n",
      "========================================\n",
      "Batch:  13920\n",
      "Cumulative Time: 5095.3044s\n",
      "Loss: 0.0187\n",
      "========================================\n",
      "Batch:  13960\n",
      "Cumulative Time: 5118.1889s\n",
      "Loss: 0.0187\n",
      "========================================\n",
      "Batch:  14000\n",
      "Cumulative Time: 5140.1573s\n",
      "Loss: 0.0187\n",
      "========================================\n",
      "Batch:  14040\n",
      "Cumulative Time: 5162.0697s\n",
      "Loss: 0.0187\n",
      "========================================\n",
      "Batch:  14080\n",
      "Cumulative Time: 5183.8612s\n",
      "Loss: 0.0187\n",
      "========================================\n",
      "Batch:  14120\n",
      "Cumulative Time: 5206.0038s\n",
      "Loss: 0.0187\n",
      "========================================\n",
      "Batch:  14160\n",
      "Cumulative Time: 5227.8825s\n",
      "Loss: 0.0187\n",
      "========================================\n",
      "Batch:  14200\n",
      "Cumulative Time: 5249.7280s\n",
      "Loss: 0.0187\n",
      "========================================\n",
      "Batch:  14240\n",
      "Cumulative Time: 5271.6765s\n",
      "Loss: 0.0187\n",
      "========================================\n",
      "Batch:  14280\n",
      "Cumulative Time: 5293.6246s\n",
      "Loss: 0.0187\n",
      "========================================\n",
      "Batch:  14320\n",
      "Cumulative Time: 5316.2634s\n",
      "Loss: 0.0187\n",
      "========================================\n",
      "Batch:  14360\n",
      "Cumulative Time: 5338.2348s\n",
      "Loss: 0.0187\n",
      "========================================\n",
      "Batch:  14400\n",
      "Cumulative Time: 5360.0297s\n",
      "Loss: 0.0187\n",
      "========================================\n",
      "Batch:  14440\n",
      "Cumulative Time: 5383.4298s\n",
      "Loss: 0.0187\n",
      "========================================\n",
      "Batch:  14480\n",
      "Cumulative Time: 5406.6078s\n",
      "Loss: 0.0187\n",
      "========================================\n",
      "Batch:  14520\n",
      "Cumulative Time: 5428.4576s\n",
      "Loss: 0.0187\n",
      "========================================\n",
      "Batch:  14560\n",
      "Cumulative Time: 5450.2781s\n",
      "Loss: 0.0187\n",
      "========================================\n",
      "Batch:  14600\n",
      "Cumulative Time: 5472.3212s\n",
      "Loss: 0.0187\n",
      "========================================\n",
      "Batch:  14640\n",
      "Cumulative Time: 5496.1649s\n",
      "Loss: 0.0187\n",
      "========================================\n",
      "Batch:  14680\n",
      "Cumulative Time: 5518.1823s\n",
      "Loss: 0.0187\n",
      "========================================\n",
      "Batch:  14720\n",
      "Cumulative Time: 5540.9081s\n",
      "Loss: 0.0187\n",
      "========================================\n",
      "Batch:  14760\n",
      "Cumulative Time: 5563.0175s\n",
      "Loss: 0.0187\n",
      "========================================\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-527d34002509>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mtime_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-23-d9cf5c93930a>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(model, train_loader, optimizer)\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'='\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \"\"\"\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(epochs):\n",
    "    time_train, train_loss = train_epoch(model, train_loader, optimizer)\n",
    "    val_loss, val_results = test_model(model, val_loader,)\n",
    "    train_loss, train_results = test_model(model, train_loader)\n",
    "    \n",
    "    print('='*60)\n",
    "    print('Epoch: {:.0f}\\nTrain Time: {:.4f}s\\nTrain Loss: {:.4f}\\nVal Loss: {:.4f}'.format(i+1, time_train, train_loss, val_loss))\n",
    "    print('='*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE/LOAD MODEL\n",
    "# path = \"./\"\n",
    "# torch.save(network.state_dict(), path)\n",
    "# network.load_state_dict(torch.load(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
